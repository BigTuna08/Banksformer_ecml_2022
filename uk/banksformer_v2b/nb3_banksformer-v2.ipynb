{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook can train a model to generate sythetic data.   \n",
    "Ensure the 'ds_suffix' matches the one used to generated the dataset (Under \"Set input dataset\" & in create_dataset notebook)  \n",
    "Parameters for generating data (seq_len, number of seqs) are near bottom (Under \"Generate Full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set input dataset and nb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_KEY_ORDER is ['tcode_num', 'dow', 'month', 'day', 'dtme', 'td_sc', 'log_amount_sc']\n",
      "LOSS_TYPES are: day - scce, dtme - scce, dow - scce, month - scce, td_sc - pdf, log_amount_sc - pdf, tcode_num - scce\n",
      "If this is not correct, edit field_config.py and re-run notebook\n"
     ]
    }
   ],
   "source": [
    "from field_config import CLOCK_DIMS, get_field_info, DATA_KEY_ORDER, LOSS_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_suffix = \"-uk\"\n",
    "nb_id = \"cond\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6983, 21, 54), (6983, 20, 7), (6983,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tensor = np.load(f\"stored_data/inp_tensor-{ds_suffix}.npy\")\n",
    "tar_tensor = np.load(f\"stored_data/tar_tensor-{ds_suffix}.npy\")\n",
    "attributes = np.load(f\"stored_data/attributes-{ds_suffix}.npy\")\n",
    "\n",
    "inp_tensor.shape, tar_tensor.shape, attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqs, n_steps, n_feat_inp = inp_tensor.shape\n",
    "n_feat_tar = tar_tensor.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import load_data_encoder\n",
    "data_encoder = load_data_encoder(ds_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and create tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_cv, inds_tr, inds_cv, targ_tr, targ_cv = train_test_split(\n",
    "    inp_tensor, np.arange(n_seqs), tar_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-29 11:56:27.712714: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(21, 54), dtype=tf.float32, name=None), TensorSpec(shape=(20, 7), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tr = tf.data.Dataset.from_tensor_slices((x_tr.astype(np.float32), targ_tr.astype(np.float32)))\n",
    "ds_cv = tf.data.Dataset.from_tensor_slices((x_cv.astype(np.float32), targ_cv.astype(np.float32)))\n",
    "\n",
    "ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import make_batches\n",
    "\n",
    "BUFFER_SIZE = ds_tr.cardinality().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf_gen(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.cast(tf.math.log(2. * np.pi), tf.float64)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "loss_scce_logit = SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "loss_scce_probit = SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "\n",
    "loss_mse = MeanSquaredError(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(tf.reduce_sum(seq, axis=2), 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_parts = []\n",
    "    loss_parts_weighted = []\n",
    "\n",
    "    for k, k_pred in pred.items():\n",
    "\n",
    "        st = FIELD_STARTS_TAR[k]\n",
    "        end = st + FIELD_DIMS_TAR[k]\n",
    "        loss_type = LOSS_TYPES[k]\n",
    "        \n",
    "\n",
    "        if loss_type == \"scce\":\n",
    "            loss_ = loss_scce_logit(real[:, :, st:end], k_pred)\n",
    "        elif loss_type == \"clock\":\n",
    "            loss_ = loss_scce_probit(real[:, :, st:end], clock_to_onehot(k, k_pred))\n",
    "        elif loss_type == \"mse\":\n",
    "            loss_ = loss_mse(real[:, :, st:end], k_pred)\n",
    "        elif loss_type == \"pdf\":\n",
    "            loss_ = -log_normal_pdf(real[:, :, st:end], k_pred[:,:,0:1], k_pred[:,:,1:2])[:,:,0]\n",
    "        else:\n",
    "            raise Exception(f\"Invalid loss type! Got loss type = {loss_type} with key = {k}. Check field_config.py for loss types\")\n",
    "            \n",
    "\n",
    "        mask = tf.math.logical_not(tf.math.equal(tf.reduce_sum(real, axis=2), 0))\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask) \n",
    "\n",
    "        loss_parts.append(loss_)\n",
    "        loss_parts_weighted.append(loss_ * LOSS_WEIGHTS[k])\n",
    "\n",
    "    return tf.reduce_sum(loss_parts_weighted), loss_parts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 31, 'dtme': 31, 'dow': 7, 'month': 12}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_lib.encoding import bulk_encode_time_value\n",
    "\n",
    "EPS_CLOCKP = 0.01\n",
    "\n",
    "CLOCKS = {}\n",
    "for k, val in CLOCK_DIMS.items():\n",
    "    CLOCKS[k] = tf.constant(bulk_encode_time_value(np.arange(val), val), dtype=tf.float32)\n",
    "\n",
    "def clock_to_probs(pt, pts):\n",
    "    \n",
    "    ds = tf.constant(pts) - pt\n",
    "    sq_ds = np.sum(tf.square(ds+EPS_CLOCKP), axis=1)\n",
    "    raw_ps = 1/ sq_ds   \n",
    "    \n",
    "    return raw_ps / np.sum(raw_ps)\n",
    "\n",
    "\n",
    "\n",
    "def clock_to_onehot(k, vals):\n",
    "    orig_shape = vals.shape\n",
    "\n",
    "    vals = tf.reshape(vals, (-1, orig_shape[-1]))\n",
    "\n",
    "    return np.array([clock_to_probs(p, CLOCKS[k]) for p in vals]).reshape(*orig_shape[:-1], -1)   \n",
    "\n",
    "\n",
    "CLOCK_DIMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Banksformer configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATIONS = {\n",
    "    \"td_sc\": \"relu\",\n",
    "    \"log_amount_sc\": \"relu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "FIELD_DIMS_IN, FIELD_STARTS_IN, FIELD_DIMS_TAR, FIELD_STARTS_TAR, FIELD_DIMS_NET, FIELD_STARTS_NET = get_field_info(ds_suffix)\n",
    "\n",
    "config[\"ORDER\"] = DATA_KEY_ORDER\n",
    "config[\"FIELD_STARTS_IN\"] = FIELD_STARTS_IN\n",
    "config[\"FIELD_DIMS_IN\"] = FIELD_DIMS_IN\n",
    "config[\"FIELD_STARTS_NET\"] = FIELD_STARTS_NET\n",
    "config[\"FIELD_DIMS_NET\"] = FIELD_DIMS_NET\n",
    "\n",
    "\n",
    "config[\"ACTIVATIONS\"] = ACTIVATIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:56\n",
      "Begin running v2b__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "Epoch 1 Batch 0 Loss 12.0507\n",
      "Epoch 1 Batch 50 Loss 10.5404\n",
      "Epoch 1 Loss 10.1559\n",
      "** on validation data loss is 9.3292\n",
      "Not recording acc: 'Transformer' object has no attribute 'acc_function'\n",
      "Time taken for 1 epoch: 98.31 secs\n",
      "\n",
      "Saving checkpoint for epoch 1 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-1\n",
      "Epoch 2 Batch 0 Loss 9.3089\n",
      "Epoch 2 Batch 50 Loss 9.2354\n",
      "Epoch 2 Loss 8.9652\n",
      "** on validation data loss is 7.0959\n",
      "Time taken for 1 epoch: 100.06 secs\n",
      "\n",
      "Saving checkpoint for epoch 2 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-2\n",
      "Epoch 3 Batch 0 Loss 7.0371\n",
      "Epoch 3 Batch 50 Loss 7.2459\n",
      "Epoch 3 Loss 7.1521\n",
      "** on validation data loss is 6.8766\n",
      "Time taken for 1 epoch: 114.98 secs\n",
      "\n",
      "Saving checkpoint for epoch 3 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-3\n",
      "Epoch 4 Batch 0 Loss 6.7960\n",
      "Epoch 4 Batch 50 Loss 6.8877\n",
      "Epoch 4 Loss 6.8305\n",
      "** on validation data loss is 6.7172\n",
      "Time taken for 1 epoch: 97.66 secs\n",
      "\n",
      "Saving checkpoint for epoch 4 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-4\n",
      "Epoch 5 Batch 0 Loss 6.7142\n",
      "Epoch 5 Batch 50 Loss 6.6954\n",
      "Epoch 5 Loss 6.6485\n",
      "** on validation data loss is 6.6321\n",
      "Time taken for 1 epoch: 110.00 secs\n",
      "\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-5\n",
      "Epoch 6 Batch 0 Loss 6.7439\n",
      "Epoch 6 Batch 50 Loss 6.5610\n",
      "Epoch 6 Loss 6.5227\n",
      "** on validation data loss is 6.4657\n",
      "Time taken for 1 epoch: 108.71 secs\n",
      "\n",
      "Saving checkpoint for epoch 6 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-6\n",
      "Epoch 7 Batch 0 Loss 6.5407\n",
      "Epoch 7 Batch 50 Loss 6.4211\n",
      "Epoch 7 Loss 6.4058\n",
      "** on validation data loss is 6.3817\n",
      "Time taken for 1 epoch: 103.99 secs\n",
      "\n",
      "Saving checkpoint for epoch 7 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-7\n",
      "Epoch 8 Batch 0 Loss 6.3740\n",
      "Epoch 8 Batch 50 Loss 6.3247\n",
      "Epoch 8 Loss 6.2918\n",
      "** on validation data loss is 6.2647\n",
      "Time taken for 1 epoch: 96.99 secs\n",
      "\n",
      "Saving checkpoint for epoch 8 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-8\n",
      "Epoch 9 Batch 0 Loss 6.2769\n",
      "Epoch 9 Batch 50 Loss 6.2037\n",
      "Epoch 9 Loss 6.1866\n",
      "** on validation data loss is 6.1398\n",
      "Time taken for 1 epoch: 97.26 secs\n",
      "\n",
      "Saving checkpoint for epoch 9 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-9\n",
      "Epoch 10 Batch 0 Loss 6.1952\n",
      "Epoch 10 Batch 50 Loss 6.1198\n",
      "Epoch 10 Loss 6.1002\n",
      "** on validation data loss is 6.0522\n",
      "Time taken for 1 epoch: 103.64 secs\n",
      "\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-10\n",
      "Epoch 11 Batch 0 Loss 6.0979\n",
      "Epoch 11 Batch 50 Loss 6.0393\n",
      "Epoch 11 Loss 6.0250\n",
      "** on validation data loss is 6.0205\n",
      "Time taken for 1 epoch: 61.06 secs\n",
      "\n",
      "Saving checkpoint for epoch 11 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-11\n",
      "Epoch 12 Batch 0 Loss 5.9031\n",
      "Epoch 12 Batch 50 Loss 5.9757\n",
      "Epoch 12 Loss 5.9626\n",
      "** on validation data loss is 5.8799\n",
      "Time taken for 1 epoch: 62.49 secs\n",
      "\n",
      "Saving checkpoint for epoch 12 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-12\n",
      "Epoch 13 Batch 0 Loss 5.8732\n",
      "Epoch 13 Batch 50 Loss 5.9049\n",
      "Epoch 13 Loss 5.9027\n",
      "** on validation data loss is 5.8583\n",
      "Time taken for 1 epoch: 83.19 secs\n",
      "\n",
      "Saving checkpoint for epoch 13 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-13\n",
      "Epoch 14 Batch 0 Loss 5.9342\n",
      "Epoch 14 Batch 50 Loss 5.8643\n",
      "Epoch 14 Loss 5.8492\n",
      "** on validation data loss is 5.8068\n",
      "Time taken for 1 epoch: 55.32 secs\n",
      "\n",
      "Saving checkpoint for epoch 14 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-14\n",
      "Epoch 15 Batch 0 Loss 5.6979\n",
      "Epoch 15 Batch 50 Loss 5.8099\n",
      "Epoch 15 Loss 5.8072\n",
      "** on validation data loss is 5.7537\n",
      "Time taken for 1 epoch: 56.75 secs\n",
      "\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-15\n",
      "Epoch 16 Batch 0 Loss 5.6831\n",
      "Epoch 16 Batch 50 Loss 5.7668\n",
      "Epoch 16 Loss 5.7629\n",
      "** on validation data loss is 5.7306\n",
      "Time taken for 1 epoch: 59.24 secs\n",
      "\n",
      "Saving checkpoint for epoch 16 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-16\n",
      "Epoch 17 Batch 0 Loss 5.7260\n",
      "Epoch 17 Batch 50 Loss 5.7241\n",
      "Epoch 17 Loss 5.7234\n",
      "** on validation data loss is 5.7168\n",
      "Time taken for 1 epoch: 56.08 secs\n",
      "\n",
      "Saving checkpoint for epoch 17 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-17\n",
      "Epoch 18 Batch 0 Loss 5.7722\n",
      "Epoch 18 Batch 50 Loss 5.6890\n",
      "Epoch 18 Loss 5.6860\n",
      "** on validation data loss is 5.7325\n",
      "Time taken for 1 epoch: 56.31 secs\n",
      "\n",
      "Saving checkpoint for epoch 18 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-18\n",
      "Epoch 19 Batch 0 Loss 5.8062\n",
      "Epoch 19 Batch 50 Loss 5.6622\n",
      "Epoch 19 Loss 5.6529\n",
      "** on validation data loss is 5.6295\n",
      "Time taken for 1 epoch: 60.66 secs\n",
      "\n",
      "Saving checkpoint for epoch 19 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-19\n",
      "Epoch 20 Batch 0 Loss 5.6841\n",
      "Epoch 20 Batch 50 Loss 5.6315\n",
      "Epoch 20 Loss 5.6198\n",
      "** on validation data loss is 5.6076\n",
      "Time taken for 1 epoch: 55.98 secs\n",
      "\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-20\n",
      "Epoch 21 Batch 0 Loss 5.6958\n",
      "Epoch 21 Batch 50 Loss 5.6137\n",
      "Epoch 21 Loss 5.5936\n",
      "** on validation data loss is 5.5824\n",
      "Time taken for 1 epoch: 50.74 secs\n",
      "\n",
      "Saving checkpoint for epoch 21 at ./checkpoints/v2b__nld_4-dm_128-nh_2-i_0-dr_0__1-opt_adam-lwi_0-bs_64--uk-cond/ckpt-21\n",
      "Epoch 22 Batch 0 Loss 5.5078\n",
      "Epoch 22 Batch 50 Loss 5.5664\n"
     ]
    }
   ],
   "source": [
    "from my_lib.BanksformerGen import Transformer\n",
    "import pickle \n",
    "\n",
    "\n",
    "all_models = []\n",
    "for_df = []\n",
    "\n",
    "\n",
    "def to_num(x):\n",
    "    try: return int(x)\n",
    "    except: return float(x)\n",
    "\n",
    "    \n",
    "def id_str_to_folder(id_str):\n",
    "    return id_str.replace(\".\", \"__\")\n",
    "beta = 1\n",
    "\n",
    "\n",
    "# moredate\n",
    "LOSS_WEIGHTS_OLD = {\n",
    " 'td_sc':1.,\n",
    " 'year': 0.5,\n",
    " 'month': 0.15,\n",
    " 'day': 0.25,\n",
    " 'dow': 0.1,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS_0 = {\n",
    " 'td_sc':1.,\n",
    " 'month': 0.015,\n",
    " 'day': 0.025,\n",
    " 'dow': 0.01,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS_MID = {\n",
    " 'td_sc':1.,\n",
    " 'month': 0.07,\n",
    " 'day': 0.1,\n",
    " 'dow': 0.04,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "\n",
    "lws = [(LOSS_WEIGHTS_0, \"0\"), (LOSS_WEIGHTS_OLD, \"moredate\")]\n",
    "\n",
    "# lws = [(LOSS_WEIGHTS_MID, \"mid\")]\n",
    "\n",
    "td_loss_fns = [(loss_mse, \"loss_mse\")]\n",
    "\n",
    "\n",
    "EPOCHS = 80\n",
    "EARLY_STOP = 2\n",
    "\n",
    "num_layers_enc = None\n",
    "dropout_rate = 0.1\n",
    "dr = dropout_rate\n",
    "opt_name = \"adam\"\n",
    "# td_loss_fn = loss_mse\n",
    "\n",
    "\n",
    "## Tuning these ! \n",
    "d_model = 128\n",
    "num_layers_dec = 4\n",
    "num_heads = 2\n",
    "bs = 64\n",
    "# lws # above\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS, lwi = lws[0]\n",
    "\n",
    "\n",
    "dff = d_model\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "\n",
    "                print(datetime.datetime.now().strftime(\"%H:%M\"))\n",
    "\n",
    "\n",
    "                transformer = Transformer(\n",
    "                    num_layers_enc=num_layers_enc, num_layers_dec=num_layers_dec,\n",
    "                    d_model=d_model,\n",
    "                    num_heads=num_heads,\n",
    "                    dff=dff,\n",
    "                    maximum_position_encoding=256,\n",
    "                   net_info = None, \n",
    "                    inp_dim = n_feat_inp,\n",
    "                    final_dim= None,\n",
    "                    config=config,\n",
    "                    rate=dr)\n",
    "\n",
    "                optimizer = tf.keras.optimizers.Adam()\n",
    "                transformer.optimizer =  optimizer\n",
    "\n",
    "\n",
    "                train_batches = make_batches(ds_tr, BUFFER_SIZE, bs)\n",
    "\n",
    "\n",
    "                transformer.loss_function = loss_function\n",
    "                LOSS_WEIGHTS[\"dtme\"] = LOSS_WEIGHTS[\"day\"]\n",
    "\n",
    "                LOSS_WEIGHTS[\"k_symbol_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "                LOSS_WEIGHTS[\"operation_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "                LOSS_WEIGHTS[\"type_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "                transformer.LOSS_WEIGHTS = LOSS_WEIGHTS\n",
    "\n",
    "                id_str = f\"v2b__nld_{num_layers_dec}-dm_{d_model}-nh_{num_heads}-i_{i}-dr_{dr}-opt_{opt_name}-lwi_{lwi}-bs_{bs}\"\n",
    "\n",
    "                print(\"Begin running\", id_str)\n",
    "                transformer.id_str = id_str\n",
    "\n",
    "\n",
    "                all_models.append(transformer)\n",
    "                transformer.compile()\n",
    "\n",
    "\n",
    "                transformer.checkpoint_path = f\"./checkpoints/{id_str_to_folder(transformer.id_str)}-{ds_suffix}-{nb_id}\"\n",
    "                transformer.ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                                           optimizer=optimizer)\n",
    "                transformer.ckpt_manager = tf.train.CheckpointManager(transformer.ckpt, \n",
    "                                                                      transformer.checkpoint_path, max_to_keep=EARLY_STOP)\n",
    "\n",
    "                if transformer.ckpt_manager.latest_checkpoint:\n",
    "                    transformer.ckpt.restore(transformer.ckpt_manager.latest_checkpoint)\n",
    "                    print('Latest checkpoint restored!!')    \n",
    "                    continue\n",
    "\n",
    "\n",
    "                transformer.fit(train_batches, x_cv, targ_cv, epochs= EPOCHS, early_stop=EARLY_STOP, print_every=50, ckpt_every = 1)\n",
    "\n",
    "                transformer.fit_time = time.time() - start\n",
    "                transformer.results[\"fit_time\"] = transformer.fit_time \n",
    "\n",
    "                with open(f\"training_history/{id_str_to_folder(transformer.id_str)}.pickle\", \"wb\") as f:\n",
    "                    pickle.dump(transformer.results, f) \n",
    "                    print(\"Wrote transformer.results to\", f.name)\n",
    "\n",
    "\n",
    "                for_df.append((num_layers_dec, d_model, num_heads, i, dr, beta, dff,\n",
    "                               np.min(transformer.results[\"val_loss\"]), opt_name, transformer.id_str))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(for_df, columns=['num_layers_dec', 'd_model', 'num_heads', 'i', \"dr\", \"beta\", \"dff\",\n",
    "                                                \"val loss\", \"opt name\",\"id_str\"]).sort_values(\"val loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df.sort_values(\"val loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = all_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(transformer.results[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate  \n",
    "Warning: Code below is not nice and should be refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_YEARS_SPAN = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import encode_time_value\n",
    "#, decode_time_value\n",
    "\n",
    "clocks = {}\n",
    "for max_val in [7, 31, 12]:\n",
    "    cmd = f\"clocks[{max_val}] = np.array([encode_time_value(val, {max_val}) for val in range({max_val})])\"\n",
    "    print(\"Running\", cmd)\n",
    "    exec(cmd)\n",
    "    \n",
    "clocks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = data_encoder.START_DATE \n",
    "START_DATE = data_encoder.START_DATE.split()[0]\n",
    "\n",
    "import calendar\n",
    "get_dtme = lambda d: calendar.monthrange(d.year, d.month)[1] - d.day\n",
    "\n",
    "if type(START_DATE) == str:\n",
    "    START_DATE = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "    \n",
    "    \n",
    "\n",
    "END_DATE = START_DATE.replace(year = START_DATE.year+ MAX_YEARS_SPAN)\n",
    "\n",
    "ALL_DATES = [START_DATE + datetime.timedelta(i) for i in range((END_DATE - START_DATE).days)]\n",
    "\n",
    "# AD = np.array([(d.month % 12, d.day % 31, d.weekday() % 7, i, d.year) for i, d in enumerate(ALL_DATES)])\n",
    "AD = np.array([(d.month % 12, d.day % 31, d.weekday() % 7, i, d.year, get_dtme(d)) for i, d in enumerate(ALL_DATES)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import create_masks\n",
    "\n",
    "    \n",
    "def reencode_net_prediction(net_name, predictions):\n",
    "    \n",
    "    date_info = {'month':12, 'day':31, 'dtme':31, 'dow':7}\n",
    "    batch_size = predictions.shape[0]\n",
    "    \n",
    "\n",
    "    if net_name in ['balance', 'td_sc', 'dss', \"log_amount_sc\"]:\n",
    "        return predictions[:, :, 0:1]\n",
    "\n",
    "\n",
    "    elif net_name in date_info.keys():\n",
    "        dim = FIELD_DIMS_NET[net_name]\n",
    "        choices = np.arange(dim)\n",
    "\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, dim)\n",
    "\n",
    "        \n",
    "        choosen =  np.array([np.random.choice(choices, p=p) for p in ps])\n",
    "        \n",
    "        x = bulk_encode_time_value(choosen, max_val=dim)\n",
    "        \n",
    "        return np.reshape(x, newshape=(batch_size, -1, 2))\n",
    "\n",
    "    \n",
    "    elif \"_num\" in net_name:\n",
    "        dim = FIELD_DIMS_NET[net_name]\n",
    "        choices = np.arange(dim)\n",
    "\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, dim)\n",
    "        \n",
    "\n",
    "        \n",
    "        choosen =  np.reshape([np.random.choice(choices, p=p) for p in ps], newshape=(batch_size, -1))\n",
    "        \n",
    "\n",
    "        return tf.one_hot(choosen, depth=dim)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f\"Got invalid net_name: {net_name}\")\n",
    "\n",
    "days_per_month = np.array([(datetime.date(1990, month, 1) - datetime.timedelta(1)).day for month in range(1,13)]) # 0 = dec\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def get_short_name(tcode):\n",
    "    return short_names[tcode]\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def get_date_str(mm, dd):\n",
    "    return f\"{mm:02d}/{dd:02d}\"\n",
    "\n",
    "\n",
    "def bulk_decode(seqs, start_dates, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "\n",
    "    ages = seqs[:, 0, :] * data_encoder.ATTR_SCALE\n",
    "    seqs = seqs[:, 1:, :]\n",
    "    assert np.sum(np.diff(ages)) == 0, f\"Bad formating, expected all entries same in each row, got {ages}\"\n",
    "\n",
    "    \n",
    "    amts = seqs[:, :, FIELD_STARTS_IN[\"log_amount_sc\"]].numpy() * data_encoder.LOG_AMOUNT_SCALE\n",
    "    amts = 10 ** amts\n",
    "    amts = np.round(amts - 1.0, 2)\n",
    "\n",
    "\n",
    "    days_passed = np.round(seqs[:, :, FIELD_STARTS_IN[\"td_sc\"]] *data_encoder.TD_SCALE ).astype(int)\n",
    "  \n",
    "\n",
    "\n",
    "    months = np.argmax(seqs[:, :, FIELD_STARTS_IN[\"month\"]: FIELD_STARTS_IN[\"month\"] + FIELD_DIMS_IN[\"month\"]], axis=-1)\n",
    "    \n",
    "    \n",
    "    days = np.argmax(seqs[:, :, FIELD_STARTS_IN[\"day\"]: FIELD_STARTS_IN[\"day\"] + FIELD_DIMS_IN[\"day\"]], axis=-1)\n",
    "    days[days==0] = days_per_month[months[days==0]]\n",
    "    months[months==0] = 12 # needs to be done after days (above)\n",
    "    date_fields = get_date_str(months, days)\n",
    "    \n",
    "    dpc = np.cumsum(days_passed, axis=1) \n",
    "    dates = np.array([[start_dates[i] + datetime.timedelta(int(d)) for d in dpc[i]]for i in range(len(start_dates))])\n",
    "    \n",
    "    \n",
    "    code_names = []\n",
    "    code_vals = []\n",
    "    for field, start_i in FIELD_STARTS_IN.items():\n",
    "        if \"_num\" in field:\n",
    "            code_names.append(field)\n",
    "            code_vals.append(np.argmax(seqs[:, :, start_i: start_i + FIELD_DIMS_IN[field]], axis=-1))\n",
    "        \n",
    "\n",
    "    ages = np.repeat(ages[:, 0:1], amts.shape[1], axis=1).astype(int)\n",
    "    \n",
    "\n",
    "    return_vals = amts, *code_vals, date_fields, days_passed, ages, dates\n",
    "    return_lbls = \"amount\", *code_names, \"date_fields\", \"days_passed\", \"age\", \"date\"\n",
    "\n",
    "    \n",
    "    if return_df_list:\n",
    "        return [pd.DataFrame.from_records(zip(*x), columns=return_lbls) for x in zip(*return_vals)]\n",
    "    \n",
    "    if return_single_df:\n",
    "        return pd.DataFrame.from_records([x for x in zip(*[x.reshape(-1) for x in return_vals])], columns=return_lbls)\n",
    "    \n",
    "    return return_vals\n",
    "\n",
    "\n",
    "\n",
    "def nearest_clock_ind(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return np.argmin(d_sq)\n",
    "\n",
    "\n",
    "def nearest_clock_enc(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return clock[np.argmin(d_sq)]\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_ind(encs, max_val):\n",
    "    batch_size = encs.shape[0]\n",
    "    inds =  np.array([nearest_clock_ind(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, 2))])\n",
    "    return inds.reshape((batch_size, -1))\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_enc(encs, max_val):\n",
    "    print(\"Encs shape\", encs.shape)\n",
    "\n",
    "    batch_size = encs.shape[0]\n",
    "    new_encs =  np.array([nearest_clock_enc(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, encs.shape[-1]))])\n",
    "    \n",
    "    print(\"new_Encs shape\", new_encs.shape)\n",
    "    \n",
    "    return new_encs.reshape((batch_size, -1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seqs(length, ages, start_dates, greedy_dates = False, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    if return_single_df and return_df_list:\n",
    "        raise Exception(\"At most one of: 'return_single_df' and 'return_df_list' can be true\")\n",
    "    \n",
    "    date_inds = np.array([(d - START_DATE).days for d in start_dates])\n",
    "    \n",
    "    max_length = length\n",
    "\n",
    "    output = np.repeat(np.array(ages)[:, None, None], repeats=n_feat_inp, axis=2) / data_encoder.ATTR_SCALE\n",
    "    \n",
    "    raw_preds = []\n",
    "    raw_preds.append(output)\n",
    "\n",
    "    date_info = None\n",
    "    \n",
    "    \n",
    "    for i in range(max_length):\n",
    "\n",
    "\n",
    "        combined_mask, dec_padding_mask = create_masks(output)\n",
    "\n",
    "\n",
    "        predictions, attn, raw_ps, date_inds, enc_preds, date_info = call_to_generate(transformer, output, \n",
    "                                                 True, \n",
    "                                                 combined_mask, \n",
    "                                                 dec_padding_mask, date_inds, date_info, greedy_dates =greedy_dates)\n",
    "\n",
    "        \n",
    "        raw_preds.append(raw_ps)\n",
    "\n",
    "        enc_preds = tf.reshape(tf.constant(enc_preds), shape=(-1,1, n_feat_inp))\n",
    "\n",
    "        output = tf.concat([output, enc_preds], axis=1)\n",
    "\n",
    "        \n",
    "    return bulk_decode(output, start_dates, return_single_df, return_df_list), output, raw_preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Forward pass through transformer\n",
    "# \n",
    "# Returns: preds, attn_w, raw_preds, inds\n",
    "# the returned preds have multiple timesteps, but we only \n",
    "# care about the last (it's the only new one)\n",
    "def call_to_generate(transformer, tar, training,\n",
    "           look_ahead_mask, dec_padding_mask, start_inds, prev_date_info=None, greedy_dates = True):\n",
    "    \n",
    "\n",
    "    ### Pass through decoder stack ###\n",
    "    dec_output, attention_weights = transformer.decoder(\n",
    "        tar, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "\n",
    "    final_output = transformer.final_layer(dec_output) \n",
    "\n",
    "    \n",
    "    \n",
    "    ### Predict each field  ###\n",
    "    preds = {}\n",
    "    raw_preds = {}\n",
    "    encoded_preds_d = {}\n",
    "    encoded_preds = []\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    for net_name in transformer.ORDER:  \n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "        pred = reencode_net_prediction(net_name, pred) # keeps time step\n",
    "        preds[net_name] = pred\n",
    "        \n",
    "        encoded_preds_d[net_name] = pred[:,-1,:] \n",
    "        encoded_preds.append(pred[:,-1,:])\n",
    "        final_output = tf.concat([final_output, pred], axis=2)\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "    pred_date = None\n",
    "    \n",
    "\n",
    "    combined_date_info, inds = raw_dates_to_reencoded(raw_preds, start_inds)\n",
    "    \n",
    "    encoded_preds_d.update(combined_date_info)\n",
    "    \n",
    "    l = [encoded_preds_d[k] for k in transformer.ORDER]\n",
    "    encoded_preds =  tf.expand_dims(tf.concat(l, axis=1), axis=1)\n",
    "    \n",
    "\n",
    "    return preds, attention_weights, raw_preds, start_inds + inds, encoded_preds, pred_date\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes raw predictions (info about predicted day, month, dow, and days passed) and start inds \n",
    "# (indicate the current date for each of the seqs) \n",
    "# Computes a number of days passed for each based on inputs (either greedily or with sampling)\n",
    "# returns the new_dates (old_dates + days passed) and their indicies\n",
    "def raw_dates_to_reencoded(raw, start_inds,  max_days = 100, greedy_decode=False):\n",
    "    \n",
    "\n",
    "    all_ps = [tf.nn.softmax(raw[k][:,-1]).numpy() for k in [\"month\", \"day\", \"dow\", \"dtme\"]]\n",
    "\n",
    "    timesteps = np.zeros(len(start_inds)).astype(int)\n",
    "\n",
    "    sc = data_encoder.TD_SCALE\n",
    "    for i, (month_ps, day_ps, dow_ps, dtme_ps, td_pred, si) in enumerate(zip(*all_ps, raw[\"td_sc\"][:,-1].numpy(), start_inds)):\n",
    "        \n",
    "        \n",
    "\n",
    "        ps = month_ps[AD[si:si+max_days,0]]*day_ps[AD[si:si+max_days,1]]*dow_ps[AD[si:si+max_days,2]] *dtme_ps[AD[si:si+max_days,-1]] * \\\n",
    "                np.exp(log_normal_pdf_gen(AD[si:si+max_days,3]-si, mean = td_pred[0]*sc, logvar=td_pred[1]*sc))\n",
    "\n",
    "\n",
    "        \n",
    "        if greedy_decode:\n",
    "            timesteps[i] = np.argmax(ps)\n",
    "        else:\n",
    "            timesteps[i] = np.random.choice(max_days, p=ps/sum(ps))\n",
    "        \n",
    "        \n",
    "    inds = start_inds + timesteps\n",
    "    \n",
    "    \n",
    "    return_ = {}\n",
    "    return_[\"td_sc\"] = tf.expand_dims(timesteps.astype(np.float32)/ data_encoder.TD_SCALE, axis=1)\n",
    "    return_[\"month\"] = bulk_encode_time_value(AD[inds, 0], 12)\n",
    "    return_[\"day\"] = bulk_encode_time_value(AD[inds, 1], 31)\n",
    "    return_[\"dow\"] = bulk_encode_time_value(AD[inds, 2], 7)\n",
    "    return_[\"dtme\"] = bulk_encode_time_value(AD[inds, -1], 31)\n",
    "    \n",
    "    \n",
    "\n",
    "    return return_, timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_dfs, seqs, raw = generate_seqs(length= 25, \n",
    "                          ages=[75, 25], \n",
    "                          start_dates=[START_DATE, START_DATE+datetime.timedelta(days=1)], \n",
    "                          greedy_dates=False,\n",
    "                          return_df_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = seqs_dfs[1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_num(self, field, code):\n",
    "    field = field.replace(\"_num\", \"\")\n",
    "    d = self.__getattribute__(f\"{field}_to_num\".upper())\n",
    "    return d[code]\n",
    "\n",
    "\n",
    "\n",
    "def get_code_from_num(self, field, num):\n",
    "    field = field.replace(\"_num\", \"\")\n",
    "    d = self.__getattribute__(f\"num_to_{field}\".upper())\n",
    "    return d[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from field_config import CAT_FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = \"k_symbol\"\n",
    "for field in CAT_FIELDS:\n",
    "    field = field.replace(\"_num\", \"\")\n",
    "    df[field] = df[field + \"_num\"].apply(lambda x: get_code_from_num(data_encoder, field, x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "n_seqs_to_generate = len(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.read_csv(f\"stored_data/final_df-{ds_suffix}.csv\", parse_dates=[\"datetime\"])\n",
    "real_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqs_to_generate = 5000\n",
    "\n",
    "\n",
    "start_date_opts = real_df.groupby(\"account_id\")[\"datetime\"].min().dt.date.to_list()\n",
    "# start_date_opts = [START_DATE + datetime.timedelta(i) for i in range(365)]\n",
    "\n",
    "start_dates = np.random.choice(start_date_opts, size=n_seqs_to_generate)\n",
    "\n",
    "\n",
    "seq_ages = np.random.choice(attributes, size=n_seqs_to_generate)\n",
    "seq_ages\n",
    "\n",
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_models)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    transformer = all_models[i]\n",
    "    \n",
    "    print(\"Begin with \", transformer.id_str)\n",
    "\n",
    "    \n",
    "    save_as = f\"generated_data/gen_{id_str_to_folder(transformer.id_str)}--{nb_id}-len_{seq_len}-v2.csv\"\n",
    "    \n",
    "    if os.path.exists(save_as):\n",
    "        print(\"**** Skipping because file already exists. File name =\", save_as, \"\\n\\n\\n\")\n",
    "\n",
    "    start = time.time()\n",
    "    full_df, seqs, raw = generate_seqs(length= seq_len, \n",
    "                                       ages=seq_ages, \n",
    "                                       start_dates= start_dates, \n",
    "                                       return_single_df=True,\n",
    "                                      greedy_dates=True)\n",
    "    \n",
    "    full_df[\"account_id\"] = np.arange(len(full_df)) // seq_len\n",
    "    \n",
    "    for field in CAT_FIELDS:\n",
    "        field = field.replace(\"_num\", \"\")\n",
    "        full_df[field] = full_df[field + \"_num\"].apply(lambda x: get_code_from_num(data_encoder, field, x))\n",
    "        \n",
    "\n",
    "    print(f\"took {time.time() - start} secs to generate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    full_df.to_csv(save_as)\n",
    "    print(\"Wrote df to\", save_as)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
