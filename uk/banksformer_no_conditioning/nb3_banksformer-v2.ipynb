{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swymtxpl7W7w"
   },
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook trains Banksformer models and generates sythetic data.   \n",
    "Parameters for generating data (seq_len, number of seqs) are near bottom (Under \"Generate Full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JjJJyJTZYebt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pXzVhU34zWEU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cCvXbPkccV1"
   },
   "source": [
    "### Set input dataset and nb_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_KEY_ORDER is ['tcode_num', 'dow', 'month', 'day', 'dtme', 'td_sc', 'log_amount_sc']\n",
      "LOSS_TYPES are: day - scce, dtme - scce, dow - scce, month - scce, td_sc - pdf, log_amount_sc - pdf, tcode_num - scce\n",
      "If this is not correct, edit field_config.py and re-run notebook\n"
     ]
    }
   ],
   "source": [
    "from field_config import CLOCK_DIMS, get_field_info, DATA_KEY_ORDER, LOSS_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_suffix = \"-uk\"\n",
    "nb_id = \"nocond\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6983, 21, 54), (6983, 20, 7), (6983,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tensor = np.load(f\"stored_data/inp_tensor-{ds_suffix}.npy\")\n",
    "tar_tensor = np.load(f\"stored_data/tar_tensor-{ds_suffix}.npy\")\n",
    "attributes = np.load(f\"stored_data/attributes-{ds_suffix}.npy\")\n",
    "\n",
    "inp_tensor.shape, tar_tensor.shape, attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqs, n_steps, n_feat_inp = inp_tensor.shape\n",
    "n_feat_tar = tar_tensor.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import load_data_encoder\n",
    "data_encoder = load_data_encoder(ds_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and create tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_cv, inds_tr, inds_cv, targ_tr, targ_cv = train_test_split(\n",
    "    inp_tensor, np.arange(n_seqs), tar_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-29 12:12:23.128926: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(21, 54), dtype=tf.float32, name=None), TensorSpec(shape=(20, 7), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tr = tf.data.Dataset.from_tensor_slices((x_tr.astype(np.float32), targ_tr.astype(np.float32)))\n",
    "ds_cv = tf.data.Dataset.from_tensor_slices((x_cv.astype(np.float32), targ_cv.astype(np.float32)))\n",
    "\n",
    "ds_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BUN_jLBTwNxk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import make_batches\n",
    "\n",
    "BUFFER_SIZE = ds_tr.cardinality().numpy()\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_batches = make_batches(ds_tr, BUFFER_SIZE, BATCH_SIZE)\n",
    "val_batches = make_batches(ds_cv, BUFFER_SIZE, BATCH_SIZE)\n",
    "\n",
    "sample_batch = next(iter(train_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SoX0-vd1hue",
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf_gen(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.cast(tf.math.log(2. * np.pi), tf.float64)\n",
    "    return  -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZOJUSB1T8GjM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError, SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "loss_scce_logit = SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "loss_scce_probit = SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction='none')\n",
    "\n",
    "loss_mse = MeanSquaredError(reduction='none')\n",
    "\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(tf.reduce_sum(seq, axis=2), 0), tf.float32)\n",
    "\n",
    "    # add extra dimensions to add the padding\n",
    "    # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_parts = []\n",
    "    loss_parts_weighted = []\n",
    "\n",
    "    for k, k_pred in pred.items():\n",
    "\n",
    "        st = FIELD_STARTS_TAR[k]\n",
    "        end = st + FIELD_DIMS_TAR[k]\n",
    "        loss_type = LOSS_TYPES[k]\n",
    "        \n",
    "\n",
    "        if loss_type == \"scce\":\n",
    "            loss_ = loss_scce_logit(real[:, :, st:end], k_pred)\n",
    "        elif loss_type == \"clock\":\n",
    "            loss_ = loss_scce_probit(real[:, :, st:end], clock_to_onehot(k, k_pred))\n",
    "        elif loss_type == \"mse\":\n",
    "            loss_ = loss_mse(real[:, :, st:end], k_pred)\n",
    "        elif loss_type == \"pdf\":\n",
    "            loss_ = -log_normal_pdf(real[:, :, st:end], k_pred[:,:,0:1], k_pred[:,:,1:2])[:,:,0]\n",
    "        else:\n",
    "            raise Exception(f\"Invalid loss type! Got loss type = {loss_type} with key = {k}. Check field_config.py for loss types\")\n",
    "\n",
    "        mask = tf.math.logical_not(tf.math.equal(tf.reduce_sum(real, axis=2), 0))\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask) \n",
    "\n",
    "        loss_parts.append(loss_)\n",
    "        loss_parts_weighted.append(loss_ * LOSS_WEIGHTS[k])\n",
    "\n",
    "    return tf.reduce_sum(loss_parts_weighted), loss_parts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 31, 'dtme': 31, 'dow': 7, 'month': 12}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_lib.encoding import bulk_encode_time_value\n",
    "\n",
    "EPS_CLOCKP = 0.01\n",
    "\n",
    "CLOCKS = {}\n",
    "for k, val in CLOCK_DIMS.items():\n",
    "    CLOCKS[k] = tf.constant(bulk_encode_time_value(np.arange(val), val), dtype=tf.float32)\n",
    "\n",
    "def clock_to_probs(pt, pts):\n",
    "    \n",
    "    ds = tf.constant(pts) - pt\n",
    "    sq_ds = np.sum(tf.square(ds+EPS_CLOCKP), axis=1)\n",
    "    raw_ps = 1/ sq_ds   \n",
    "    \n",
    "    return raw_ps / np.sum(raw_ps)\n",
    "\n",
    "\n",
    "\n",
    "def clock_to_onehot(k, vals):\n",
    "    orig_shape = vals.shape\n",
    "\n",
    "    vals = tf.reshape(vals, (-1, orig_shape[-1]))\n",
    "\n",
    "    return np.array([clock_to_probs(p, CLOCKS[k]) for p in vals]).reshape(*orig_shape[:-1], -1)   \n",
    "\n",
    "\n",
    "CLOCK_DIMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Banksformer configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATIONS = {\n",
    "    \"td_sc\": \"relu\",\n",
    "    \"log_amount_sc\": \"relu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lnJn5SLA2ahP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EARLY_STOP = 2\n",
    "EPOCHS = 80\n",
    "\n",
    "\n",
    "num_layers_enc = None\n",
    "opt_name = \"adam\"\n",
    "dff = 128\n",
    "num_layers_dec = 4\n",
    "d_model = 128\n",
    "num_heads = 2\n",
    "dropout_rate = 0.1\n",
    "dr = dropout_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "# ONE_HOT_DIMS, FIELD_DIMS, FIELD_STARTS, FIELD_DIMS_TAR, FIELD_STARTS_TAR = get_field_info(ds_suffix)\n",
    "FIELD_DIMS_IN, FIELD_STARTS_IN, FIELD_DIMS_TAR, FIELD_STARTS_TAR, FIELD_DIMS_NET, FIELD_STARTS_NET = get_field_info(ds_suffix)\n",
    "\n",
    "config[\"ORDER\"] = DATA_KEY_ORDER\n",
    "config[\"FIELD_STARTS_IN\"] = FIELD_STARTS_IN\n",
    "config[\"FIELD_DIMS_IN\"] = FIELD_DIMS_IN\n",
    "config[\"FIELD_STARTS_NET\"] = FIELD_STARTS_NET\n",
    "config[\"FIELD_DIMS_NET\"] = FIELD_DIMS_NET\n",
    "\n",
    "\n",
    "config[\"ACTIVATIONS\"] = ACTIVATIONS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bbvmaKNiznHZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:12\n",
      "Begin running v2b-nc__nld_4-dm_128-nh_2-i_0-dr_0.1-opt_adam-lwi_0-bs_64\n",
      "Epoch 1 Batch 0 Loss 11.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/lq/672qmyl5749_q6568_r1bgmh0000gp/T/ipykernel_82090/2994714777.py\", line 129, in <module>\n",
      "    transformer.fit(train_batches, x_cv, targ_cv, epochs= EPOCHS, early_stop=EARLY_STOP, print_every=50, ckpt_every = 1)\n",
      "  File \"/Users/kylenickerson/Dropbox/Mac/Desktop/verafin_proj_big/banksformer_ecml2022/uk/banksformer_v2b_no_conditioning/my_lib/BanksformerGen.py\", line 228, in fit\n",
      "    self.train_step(inp, tar)\n",
      "  File \"/Users/kylenickerson/Dropbox/Mac/Desktop/verafin_proj_big/banksformer_ecml2022/uk/banksformer_v2b_no_conditioning/my_lib/BanksformerGen.py\", line 192, in train_step\n",
      "    loss, *_ = self.loss_function(tar, predictions)\n",
      "  File \"/var/folders/lq/672qmyl5749_q6568_r1bgmh0000gp/T/ipykernel_82090/2187165360.py\", line 52, in loss_function\n",
      "    loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 1082, in op_dispatch_handler\n",
      "    return dispatch_target(*args, **kwargs)\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\", line 2311, in reduce_sum\n",
      "    return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\", line 2323, in reduce_sum_with_dims\n",
      "    gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 11192, in _sum\n",
      "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/inspect.py\", line 755, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/posixpath.py\", line 392, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/posixpath.py\", line 426, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/Users/kylenickerson/opt/anaconda3/lib/python3.9/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/var/folders/lq/672qmyl5749_q6568_r1bgmh0000gp/T/ipykernel_82090/2994714777.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEARLY_STOP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Mac/Desktop/verafin_proj_big/banksformer_ecml2022/uk/banksformer_v2b_no_conditioning/my_lib/BanksformerGen.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_batches, x_cv, y_cv, epochs, early_stop, print_every, ckpt_every, mid_epoch_updates)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Mac/Desktop/verafin_proj_big/banksformer_ecml2022/uk/banksformer_v2b_no_conditioning/my_lib/BanksformerGen.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, inp, tar)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lq/672qmyl5749_q6568_r1bgmh0000gp/T/ipykernel_82090/2187165360.py\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(real, pred)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mloss_\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[1;32m   2312\u001b[0m                               _ReductionDims(input_tensor, axis))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   2322\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2323\u001b[0;31m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[1;32m   2324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m  11191\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11192\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m  11193\u001b[0m         _ctx, \"Sum\", name, input, axis, \"keep_dims\", keep_dims)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2064\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from my_lib.BanksformerGen import Transformer\n",
    "import pickle \n",
    "\n",
    "\n",
    "all_models = []\n",
    "for_df = []\n",
    "\n",
    "\n",
    "def to_num(x):\n",
    "    try: return int(x)\n",
    "    except: return float(x)\n",
    "\n",
    "    \n",
    "def id_str_to_folder(id_str):\n",
    "    return id_str.replace(\".\", \"__\")\n",
    "beta = 1\n",
    "\n",
    "\n",
    "# moredate\n",
    "LOSS_WEIGHTS_OLD = {\n",
    " 'td_sc':1.,\n",
    " 'year': 0.5,\n",
    " 'month': 0.15,\n",
    " 'day': 0.25,\n",
    " 'dow': 0.1,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "LOSS_WEIGHTS_0 = {\n",
    " 'td_sc':1.,\n",
    " 'month': 0.015,\n",
    " 'day': 0.025,\n",
    " 'dow': 0.01,\n",
    " 'tcode_num': 1.,\n",
    " 'log_amount_sc': 2.}\n",
    "\n",
    "\n",
    "\n",
    "lws = [(LOSS_WEIGHTS_0, \"0\"), (LOSS_WEIGHTS_OLD, \"moredate\")]\n",
    "\n",
    "td_loss_fns = [(loss_mse, \"loss_mse\")]\n",
    "\n",
    "\n",
    "EPOCHS = 80\n",
    "EARLY_STOP = 2\n",
    "\n",
    "num_layers_enc = None\n",
    "dropout_rate = 0.1\n",
    "dr = dropout_rate\n",
    "opt_name = \"adam\"\n",
    "# td_loss_fn = loss_mse\n",
    "\n",
    "\n",
    "## Tuning these ! \n",
    "d_model = 128\n",
    "num_layers_dec = 4\n",
    "num_heads = 2\n",
    "bs = 64\n",
    "# lws # above\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dff = d_model\n",
    "\n",
    "LOSS_WEIGHTS, lwi = lws[0]\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    print(datetime.datetime.now().strftime(\"%H:%M\"))\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    transformer = Transformer(\n",
    "        num_layers_enc=num_layers_enc, num_layers_dec=num_layers_dec,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dff=dff,\n",
    "        maximum_position_encoding=256,\n",
    "       net_info = None, \n",
    "        inp_dim = n_feat_inp,\n",
    "        final_dim= None,\n",
    "        config=config,\n",
    "        rate=dr)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    transformer.optimizer =  optimizer\n",
    "\n",
    "\n",
    "    transformer.loss_function = loss_function\n",
    "    LOSS_WEIGHTS[\"dtme\"] = LOSS_WEIGHTS[\"day\"]\n",
    "\n",
    "    LOSS_WEIGHTS[\"k_symbol_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "    LOSS_WEIGHTS[\"operation_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "    LOSS_WEIGHTS[\"type_num\"] = LOSS_WEIGHTS[\"tcode_num\"]\n",
    "    transformer.LOSS_WEIGHTS = LOSS_WEIGHTS\n",
    "\n",
    "    id_str = f\"v2b-nc__nld_{num_layers_dec}-dm_{d_model}-nh_{num_heads}-i_{i}-dr_{dr}-opt_{opt_name}-lwi_{lwi}-bs_{bs}\"\n",
    "\n",
    "    print(\"Begin running\", id_str)\n",
    "    transformer.id_str = id_str\n",
    "\n",
    "\n",
    "    all_models.append(transformer)\n",
    "    transformer.compile()\n",
    "\n",
    "\n",
    "    transformer.checkpoint_path = f\"./checkpoints/{id_str_to_folder(transformer.id_str)}-{ds_suffix}-{nb_id}\"\n",
    "    transformer.ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                               optimizer=optimizer)\n",
    "    transformer.ckpt_manager = tf.train.CheckpointManager(transformer.ckpt, \n",
    "                                                          transformer.checkpoint_path, max_to_keep=EARLY_STOP)\n",
    "\n",
    "    if transformer.ckpt_manager.latest_checkpoint:\n",
    "        transformer.ckpt.restore(transformer.ckpt_manager.latest_checkpoint)\n",
    "        print('Latest checkpoint restored!!')    \n",
    "        continue\n",
    "\n",
    "    transformer.fit(train_batches, x_cv, targ_cv, epochs= EPOCHS, early_stop=EARLY_STOP, print_every=50, ckpt_every = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    transformer.fit_time = time.time() - start\n",
    "    transformer.results[\"fit_time\"] = transformer.fit_time \n",
    "\n",
    "    with open(f\"training_history/{id_str_to_folder(transformer.id_str)}.pickle\", \"wb\") as f:\n",
    "        pickle.dump(transformer.results, f) \n",
    "        print(\"Wrote transformer.results to\", f.name)\n",
    "\n",
    "\n",
    "    for_df.append((num_layers_dec, d_model, num_heads, i, dr, beta, dff,\n",
    "                   np.min(transformer.results[\"val_loss\"]), opt_name, transformer.id_str))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(for_df, columns=['num_layers_dec', 'd_model', 'num_heads', 'i', \"dr\", \"beta\", \"dff\",\n",
    "                                                \"val loss\", \"opt name\",\"id_str\"]).sort_values(\"val loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_colwidth', None, \"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(df.sort_values(\"val loss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = all_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(transformer.results[\"val_loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate  \n",
    "Warning: Code below is not nice and should be refactored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_YEARS_SPAN = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.encoding import encode_time_value\n",
    "\n",
    "clocks = {}\n",
    "for max_val in [7, 31, 12]:\n",
    "    cmd = f\"clocks[{max_val}] = np.array([encode_time_value(val, {max_val}) for val in range({max_val})])\"\n",
    "    print(\"Running\", cmd)\n",
    "    exec(cmd)\n",
    "    \n",
    "clocks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = data_encoder.START_DATE \n",
    "START_DATE = data_encoder.START_DATE.split()[0]\n",
    "\n",
    "import calendar\n",
    "get_dtme = lambda d: calendar.monthrange(d.year, d.month)[1] - d.day\n",
    "\n",
    "if type(START_DATE) == str:\n",
    "    START_DATE = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\").date()\n",
    "    \n",
    "    \n",
    "\n",
    "END_DATE = START_DATE.replace(year = START_DATE.year+ MAX_YEARS_SPAN)\n",
    "\n",
    "ALL_DATES = [START_DATE + datetime.timedelta(i) for i in range((END_DATE - START_DATE).days)]\n",
    "\n",
    "# AD = np.array([(d.month % 12, d.day % 31, d.weekday() % 7, i, d.year) for i, d in enumerate(ALL_DATES)])\n",
    "AD = np.array([(d.month % 12, d.day % 31, d.weekday() % 7, i, d.year, get_dtme(d)) for i, d in enumerate(ALL_DATES)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_lib.transformer_core import create_masks\n",
    "\n",
    "    \n",
    "def reencode_net_prediction(net_name, predictions):\n",
    "    \n",
    "    date_info = {'month':12, 'day':31, 'dtme':31, 'dow':7}\n",
    "    batch_size = predictions.shape[0]\n",
    "\n",
    "    if net_name in ['balance', 'td_sc', 'dss', \"log_amount_sc\"]:\n",
    "        return predictions[:, :, 0:1]\n",
    "\n",
    "\n",
    "    elif net_name in date_info.keys():\n",
    "        dim = FIELD_DIMS_NET[net_name]\n",
    "        choices = np.arange(dim)\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, dim)\n",
    "        \n",
    "\n",
    "        choosen =  np.array([np.random.choice(choices, p=p) for p in ps])\n",
    "\n",
    "        x = bulk_encode_time_value(choosen, max_val=dim)\n",
    "        \n",
    "        return np.reshape(x, newshape=(batch_size, -1, 2))\n",
    "        \n",
    "    \n",
    "    elif \"_num\" in net_name:\n",
    "        dim = FIELD_DIMS_NET[net_name]\n",
    "        choices = np.arange(dim)\n",
    "        ps = tf.nn.softmax(predictions, axis=2).numpy().reshape(-1, dim)\n",
    "        \n",
    "\n",
    "        choosen =  np.reshape([np.random.choice(choices, p=p) for p in ps], newshape=(batch_size, -1))\n",
    "\n",
    "        return tf.one_hot(choosen, depth=dim)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f\"Got invalid net_name: {net_name}\")\n",
    "\n",
    "days_per_month = np.array([(datetime.date(1990, month, 1) - datetime.timedelta(1)).day for month in range(1,13)]) # 0 = dec\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def get_short_name(tcode):\n",
    "    return short_names[tcode]\n",
    "\n",
    "@np.vectorize\n",
    "def get_date_str(mm, dd):\n",
    "    return f\"{mm:02d}/{dd:02d}\"\n",
    "\n",
    "\n",
    "def bulk_decode(seqs, start_dates, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "\n",
    "    ages = seqs[:, 0, :] * data_encoder.ATTR_SCALE\n",
    "    seqs = seqs[:, 1:, :]\n",
    "    assert np.sum(np.diff(ages)) == 0, f\"Bad formating, expected all entries same in each row, got {ages}\"\n",
    "\n",
    "    \n",
    "    amts = seqs[:, :, FIELD_STARTS_IN[\"log_amount_sc\"]].numpy() * data_encoder.LOG_AMOUNT_SCALE\n",
    "    amts = 10 ** amts\n",
    "    amts = np.round(amts - 1.0, 2)\n",
    "\n",
    "\n",
    "    days_passed = np.round(seqs[:, :, FIELD_STARTS_IN[\"td_sc\"]] *data_encoder.TD_SCALE ).astype(int)\n",
    "  \n",
    "\n",
    "\n",
    "    months = np.argmax(seqs[:, :, FIELD_STARTS_IN[\"month\"]: FIELD_STARTS_IN[\"month\"] + FIELD_DIMS_IN[\"month\"]], axis=-1)\n",
    "    \n",
    "    \n",
    "    days = np.argmax(seqs[:, :, FIELD_STARTS_IN[\"day\"]: FIELD_STARTS_IN[\"day\"] + FIELD_DIMS_IN[\"day\"]], axis=-1)\n",
    "    days[days==0] = days_per_month[months[days==0]]\n",
    "    months[months==0] = 12 # needs to be done after days (above)\n",
    "    date_fields = get_date_str(months, days)\n",
    "    \n",
    "    dpc = np.cumsum(days_passed, axis=1) \n",
    "    dates = np.array([[start_dates[i] + datetime.timedelta(int(d)) for d in dpc[i]]for i in range(len(start_dates))])\n",
    "    \n",
    "    \n",
    "    code_names = []\n",
    "    code_vals = []\n",
    "    for field, start_i in FIELD_STARTS_IN.items():\n",
    "        if \"_num\" in field:\n",
    "            code_names.append(field)\n",
    "            code_vals.append(np.argmax(seqs[:, :, start_i: start_i + FIELD_DIMS_IN[field]], axis=-1))\n",
    "        \n",
    "\n",
    "\n",
    "    ages = np.repeat(ages[:, 0:1], amts.shape[1], axis=1).astype(int)\n",
    "    \n",
    "\n",
    "    return_vals = amts, *code_vals, date_fields, days_passed, ages, dates\n",
    "    return_lbls = \"amount\", *code_names, \"date_fields\", \"days_passed\", \"age\", \"date\"\n",
    "\n",
    "\n",
    "    if return_df_list:\n",
    "        return [pd.DataFrame.from_records(zip(*x), columns=return_lbls) for x in zip(*return_vals)]\n",
    "    \n",
    "    if return_single_df:\n",
    "        return pd.DataFrame.from_records([x for x in zip(*[x.reshape(-1) for x in return_vals])], columns=return_lbls)\n",
    "    \n",
    "    return return_vals\n",
    "\n",
    "\n",
    "\n",
    "def nearest_clock_ind(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return np.argmin(d_sq)\n",
    "\n",
    "\n",
    "def nearest_clock_enc(enc, max_val):\n",
    "    clock = clocks[max_val]\n",
    "    diffs = clock - enc\n",
    "    d_sq =  np.sum(diffs**2, axis=1)\n",
    "    return clock[np.argmin(d_sq)]\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_ind(encs, max_val):\n",
    "    batch_size = encs.shape[0]\n",
    "    inds =  np.array([nearest_clock_ind(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, 2))])\n",
    "    return inds.reshape((batch_size, -1))\n",
    "\n",
    "\n",
    "def bulk_nearest_clock_enc(encs, max_val):\n",
    "    print(\"Encs shape\", encs.shape)\n",
    "\n",
    "    batch_size = encs.shape[0]\n",
    "    new_encs =  np.array([nearest_clock_enc(enc, max_val) \n",
    "                      for enc in tf.reshape(encs, shape=(-1, encs.shape[-1]))])\n",
    "    \n",
    "    print(\"new_Encs shape\", new_encs.shape)\n",
    "    \n",
    "    return new_encs.reshape((batch_size, -1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seqs(length, ages, start_dates, greedy_dates = False, return_single_df=False, return_df_list=False):\n",
    "    \n",
    "    if return_single_df and return_df_list:\n",
    "        raise Exception(\"At most one of: 'return_single_df' and 'return_df_list' can be true\")\n",
    "    \n",
    "    date_inds = np.array([(d - START_DATE).days for d in start_dates])\n",
    "    \n",
    "    max_length = length\n",
    "\n",
    "    output = np.repeat(np.array(ages)[:, None, None], repeats=n_feat_inp, axis=2) / data_encoder.ATTR_SCALE\n",
    "    \n",
    "    raw_preds = []\n",
    "    raw_preds.append(output)\n",
    "\n",
    "    date_info = None\n",
    "    \n",
    "    \n",
    "    for i in range(max_length):\n",
    "\n",
    "\n",
    "        combined_mask, dec_padding_mask = create_masks(output)\n",
    "\n",
    "       \n",
    "        predictions, attn, raw_ps, date_inds, enc_preds, date_info = call_to_generate(transformer, output, \n",
    "                                                 True, \n",
    "                                                 combined_mask, \n",
    "                                                 dec_padding_mask, date_inds, date_info, greedy_dates =greedy_dates)\n",
    "\n",
    "        \n",
    "        raw_preds.append(raw_ps)\n",
    "\n",
    "        enc_preds = tf.reshape(tf.constant(enc_preds), shape=(-1,1, n_feat_inp))\n",
    "\n",
    "        output = tf.concat([output, enc_preds], axis=1)\n",
    "\n",
    "        \n",
    "    return bulk_decode(output, start_dates, return_single_df, return_df_list), output, raw_preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Forward pass through transformer\n",
    "# \n",
    "# Returns: preds, attn_w, raw_preds, inds\n",
    "# the returned preds have multiple timesteps, but we only \n",
    "# care about the last (it's the only new one)\n",
    "def call_to_generate(transformer, tar, training,\n",
    "           look_ahead_mask, dec_padding_mask, start_inds, prev_date_info=None, greedy_dates = True):\n",
    "    \n",
    "\n",
    "    ### Pass through decoder stack ###\n",
    "    dec_output, attention_weights = transformer.decoder(\n",
    "        tar, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "\n",
    "    final_output = transformer.final_layer(dec_output) \n",
    "\n",
    "    \n",
    "    \n",
    "    ### Predict each field  ###\n",
    "    preds = {}\n",
    "    raw_preds = {}\n",
    "    encoded_preds_d = {}\n",
    "    encoded_preds = []\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    for net_name in transformer.ORDER:  \n",
    "#         print(\"net =\", net_name)\n",
    "        pred = transformer.__getattribute__(net_name)(final_output)\n",
    "        raw_preds[net_name] = pred\n",
    "        \n",
    "        pred = reencode_net_prediction(net_name, pred) # keeps time step\n",
    "        preds[net_name] = pred\n",
    "        \n",
    "        \n",
    "        encoded_preds_d[net_name] = pred[:,-1,:] \n",
    "        encoded_preds.append(pred[:,-1,:])\n",
    "#         encoded_preds.append(pred[:,-1,:])\n",
    "        \n",
    "        ### removed for non - conditional verstion ###\n",
    "\n",
    "#         final_output = tf.concat([final_output, pred], axis=2)\n",
    "            \n",
    "\n",
    "    pred_date = None # not used\n",
    "    \n",
    "    \n",
    "    combined_date_info, inds = raw_dates_to_reencoded(raw_preds, start_inds)\n",
    "    \n",
    "    encoded_preds_d.update(combined_date_info)\n",
    "    \n",
    "    l = [encoded_preds_d[k] for k in transformer.ORDER]\n",
    "    encoded_preds =  tf.expand_dims(tf.concat(l, axis=1), axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return preds, attention_weights, raw_preds, start_inds + inds, encoded_preds, pred_date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes raw predictions (info about predicted day, month, dow, and days passed) and start inds \n",
    "# (indicate the current date for each of the seqs) \n",
    "# Computes a number of days passed for each based on inputs (either greedily or with sampling)\n",
    "# returns the new_dates (old_dates + days passed) and their indicies\n",
    "def raw_dates_to_reencoded(raw, start_inds,  max_days = 100, greedy_decode=False):\n",
    "    \n",
    "    all_ps = [tf.nn.softmax(raw[k][:,-1]).numpy() for k in [\"month\", \"day\", \"dow\", \"dtme\"]]\n",
    "\n",
    "    timesteps = np.zeros(len(start_inds)).astype(int)\n",
    "\n",
    "    sc = data_encoder.TD_SCALE\n",
    "    for i, (month_ps, day_ps, dow_ps, dtme_ps, td_pred, si) in enumerate(zip(*all_ps, raw[\"td_sc\"][:,-1].numpy(), start_inds)):\n",
    "        \n",
    "        \n",
    "\n",
    "        ps = month_ps[AD[si:si+max_days,0]]*day_ps[AD[si:si+max_days,1]]*dow_ps[AD[si:si+max_days,2]] *dtme_ps[AD[si:si+max_days,-1]] * \\\n",
    "                np.exp(log_normal_pdf_gen(AD[si:si+max_days,3]-si, mean = td_pred[0]*sc, logvar=td_pred[1]*sc))\n",
    "#                 pmf(max(PMF_EPS, l_pred)*data_encoder.TD_SCALE, AD[si:si+max_days,3]-si ) \n",
    "\n",
    "        \n",
    "        if greedy_decode:\n",
    "            timesteps[i] = np.argmax(ps)\n",
    "        else:\n",
    "            timesteps[i] = np.random.choice(max_days, p=ps/sum(ps))\n",
    "        \n",
    "        \n",
    "    inds = start_inds + timesteps\n",
    "    \n",
    "    \n",
    "    return_ = {}\n",
    "    return_[\"td_sc\"] = tf.expand_dims(timesteps.astype(np.float32)/ data_encoder.TD_SCALE, axis=1)\n",
    "    return_[\"month\"] = bulk_encode_time_value(AD[inds, 0], 12)\n",
    "    return_[\"day\"] = bulk_encode_time_value(AD[inds, 1], 31)\n",
    "    return_[\"dow\"] = bulk_encode_time_value(AD[inds, 2], 7)\n",
    "    return_[\"dtme\"] = bulk_encode_time_value(AD[inds, -1], 31)\n",
    "    \n",
    "    \n",
    "\n",
    "    return return_, timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs_dfs, seqs, raw = generate_seqs(length= 25, \n",
    "                          ages=[75, 25], \n",
    "                          start_dates=[START_DATE, START_DATE+datetime.timedelta(days=1)], \n",
    "                          greedy_dates=False,\n",
    "                          return_df_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = seqs_dfs[1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_num(self, field, code):\n",
    "    field = field.replace(\"_num\", \"\")\n",
    "    d = self.__getattribute__(f\"{field}_to_num\".upper())\n",
    "    return d[code]\n",
    "\n",
    "\n",
    "\n",
    "def get_code_from_num(self, field, num):\n",
    "    field = field.replace(\"_num\", \"\")\n",
    "    d = self.__getattribute__(f\"num_to_{field}\".upper())\n",
    "    return d[num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from field_config import CAT_FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = \"k_symbol\"\n",
    "for field in CAT_FIELDS:\n",
    "    field = field.replace(\"_num\", \"\")\n",
    "    df[field] = df[field + \"_num\"].apply(lambda x: get_code_from_num(data_encoder, field, x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "n_seqs_to_generate = len(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_df = pd.read_csv(f\"stored_data/final_df-{ds_suffix}.csv\", parse_dates=[\"datetime\"])\n",
    "real_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqs_to_generate = 5000\n",
    "\n",
    "\n",
    "start_date_opts = real_df.groupby(\"account_id\")[\"datetime\"].min().dt.date.to_list()\n",
    "# start_date_opts = [START_DATE + datetime.timedelta(i) for i in range(365)]\n",
    "\n",
    "start_dates = np.random.choice(start_date_opts, size=n_seqs_to_generate)\n",
    "\n",
    "\n",
    "seq_ages = np.random.choice(attributes, size=n_seqs_to_generate)\n",
    "seq_ages\n",
    "\n",
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_models)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    transformer = all_models[i]\n",
    "    \n",
    "    print(\"Begin with \", transformer.id_str)\n",
    "    \n",
    "    save_as = f\"generated_data/gen_{id_str_to_folder(transformer.id_str)}--{nb_id}-len_{seq_len}-v2.csv\"\n",
    "    \n",
    "    if os.path.exists(save_as):\n",
    "        print(\"**** Skipping because file already exists. File name =\", save_as, \"\\n\\n\\n\")\n",
    "        continue\n",
    "\n",
    "    start = time.time()\n",
    "    full_df, seqs, raw = generate_seqs(length= seq_len, \n",
    "                                       ages=seq_ages, \n",
    "                                       start_dates= start_dates, \n",
    "                                       return_single_df=True,\n",
    "                                      greedy_dates=True)\n",
    "    \n",
    "    full_df[\"account_id\"] = np.arange(len(full_df)) // seq_len\n",
    "    \n",
    "    for field in CAT_FIELDS:\n",
    "        field = field.replace(\"_num\", \"\")\n",
    "        full_df[field] = full_df[field + \"_num\"].apply(lambda x: get_code_from_num(data_encoder, field, x))\n",
    "        \n",
    "\n",
    "    print(f\"took {time.time() - start} secs to generate\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    full_df.to_csv(save_as)\n",
    "    print(\"Wrote df to\", save_as)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
